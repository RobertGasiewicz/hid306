{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Algorithm using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network is, a directed graph, organized by layers and layers are created by number of interconnected neurons (or nodes). Every neuron in a layer is connected with all the neurons from previous layer; there will be no interaction of neurons within a layer. The performance of a Neural Network is measured using cost or error function and the dependent input weight variables. Forward-propagation and back-propagation are two techniques, neural network uses repeatedly until all the input variables are adjusted or calibrated to predict accurate output. During, forward-propagation, information moves in forward direction and passes through all the layers by applying certain weights to the input parameters. Back-propagation method minimizes the error in the weights by applying an algorithm called gradient descent at each iteration step. We have used TensorFlow python library to predict the sale price of housing dataset using simple feed-forward neural network. TensorFlow uses tensors, special multi-dimensional arrays to store the datasets for easier linear algebra and vector calculus operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mcheruvu/notebook/code\n",
      "\n",
      "The train data has 1460 rows and 307 columns\n",
      "The test data has 1459 rows and 306 columns\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "print(\"\")\n",
    "\n",
    "train = pd.read_csv(\"../data/train_after_feature_engineering.csv\")\n",
    "test = pd.read_csv(\"../data/test_after_feature_engineering.csv\")\n",
    "\n",
    "print ('The train data has {0} rows and {1} columns'.format(train.shape[0],train.shape[1]))    \n",
    "print ('The test data has {0} rows and {1} columns'.format(test.shape[0],test.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"]) # log(SalePrice) + 1\n",
    "\n",
    "# Shuffle data\n",
    "train = train.sample(frac=1)\n",
    "\n",
    "# Split into training, cross validation, and testing datasets\n",
    "train, cv1, cv2 = np.split(train,\n",
    "                        [int(.6 * len(train)), \n",
    "                         int(.8 * len(train))])\n",
    "\n",
    "# Convert into numpy arrays\n",
    "x_train = train.drop(['SalePrice', 'Id'], axis=1).as_matrix().astype(np.float32)\n",
    "y_train = train['SalePrice'].as_matrix().astype(np.float32).reshape((np.shape(x_train)[0], 1))\n",
    "\n",
    "x_cv1 = cv1.drop(['SalePrice', 'Id'], axis=1).as_matrix().astype(np.float32)\n",
    "y_cv1 = cv1['SalePrice'].as_matrix().astype(np.float32).reshape((np.shape(cv1)[0], 1))\n",
    "\n",
    "x_cv2 = cv2.drop(['SalePrice', 'Id'], axis=1).as_matrix().astype(np.float32)\n",
    "y_cv2 = cv2['SalePrice'].as_matrix().astype(np.float32).reshape((np.shape(cv2)[0], 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = np.shape(x_train)[0]\n",
    "cv1_size = np.shape(x_cv1)[0]\n",
    "cv2_size = np.shape(x_cv2)[0]\n",
    "\n",
    "num_features = np.shape(x_train)[1]\n",
    "num_hidden = 16 # Number of activation units in the hidden layer\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input\n",
    "    tf_train_dataset = tf.constant(x_train, dtype=tf.float32)\n",
    "    tf_train_labels = tf.constant(y_train, dtype=tf.float32)\n",
    "    \n",
    "    tf_valid_dataset = tf.constant(x_cv1)\n",
    "    tf_test_dataset = tf.constant(x_cv2)\n",
    "    \n",
    "    # Variables\n",
    "    weights_1 = tf.Variable(tf.truncated_normal(\n",
    "        [num_features, num_hidden]), dtype=tf.float32, name=\"layer1_weights\")\n",
    "    \n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden]), dtype=tf.float32, name=\"layer1_biases\")\n",
    "    \n",
    "    weights_2 = tf.Variable(tf.truncated_normal(\n",
    "        [num_hidden, 1]), dtype = tf.float32, name=\"layer2_weights\")\n",
    "    \n",
    "    biases_2 = tf.Variable(tf.zeros([1]), dtype=tf.float32, name=\"layer2_biases\")\n",
    "    \n",
    "    steps = tf.Variable(0)\n",
    "    \n",
    "    # Model\n",
    "    def model(x, train=False):\n",
    "        hidden = tf.nn.relu(tf.matmul(x, weights_1) + biases_1)\n",
    "        return tf.matmul(hidden, weights_2) + biases_2\n",
    "    #end def\n",
    "    \n",
    "    # Loss Computation\n",
    "    train_prediction = model(tf_train_dataset)\n",
    "    loss = 0.5 * tf.reduce_mean(tf.squared_difference(tf_train_labels, train_prediction))\n",
    "    cost = tf.sqrt(loss)\n",
    "    \n",
    "    # Optimizer\n",
    "    # Exponential decay of learning rate\n",
    "    learning_rate = tf.train.exponential_decay(0.06, steps, 5000, 0.70, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost, global_step=steps)\n",
    "    \n",
    "    # Predictions\n",
    "    valid_prediction = model(tf_valid_dataset)\n",
    "    test_prediction = model(tf_test_dataset)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "#end with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Test Sale Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Cost at step 0: 10791.43\n",
      "Validation loss: 4781325.50\n",
      "Cost at step 5000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 10000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 15000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 20000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 25000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 30000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 35000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 40000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 45000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 50000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 55000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 60000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 65000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 70000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 75000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 80000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 85000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 90000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 95000: 0.27\n",
      "Validation loss: 0.20\n",
      "Cost at step 100000: 0.27\n",
      "Validation loss: 0.20\n",
      "Test loss: 5.29\n",
      "Making predictions...\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "def accuracy(prediction, labels):\n",
    "    return 0.5 * np.sqrt(((prediction - labels) ** 2).mean(axis=None))\n",
    "#end def\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, c, predictions = sess.run([optimizer, cost, train_prediction])\n",
    "        if (step % 5000 == 0):\n",
    "            print('Cost at step %d: %.2f' % (step, c))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation loss: %.2f' % accuracy(valid_prediction.eval(), y_cv1))\n",
    "        #end if\n",
    "    #end for\n",
    "    \n",
    "    t_pred = test_prediction.eval()\n",
    "    \n",
    "    print('Test loss: %.2f' % accuracy(t_pred, y_cv2))    \n",
    "     \n",
    "    print(\"Making predictions...\")\n",
    "    \n",
    "    x = test.drop('Id', axis=1).as_matrix().astype(dtype=np.float32)\n",
    "    \n",
    "    hidden = tf.nn.relu(tf.matmul(x, weights_1) + biases_1)\n",
    "    \n",
    "    y = tf.cast((tf.matmul(hidden, weights_2) + biases_2), dtype=tf.uint16).eval()\n",
    "    \n",
    "    test['SalePrice'] = y\n",
    "    \n",
    "    output = test[['Id', 'SalePrice']]    \n",
    "    \n",
    "#end with\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...file saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#df_predict = pd.DataFrame({'Id': test[\"Id\"], 'SalePrice': np.exp(test[\"SalePrice\"]) - 1.0})\n",
    "#df_predict = pd.DataFrame({'Id': id_vector, 'SalePrice': sale_price_vector})\n",
    "\n",
    "output.to_csv('../data/kaggle_python_neural_network.csv', header=True, index=False)\n",
    "\n",
    "print(\"...file saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
